{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e156e22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'Class_project_Early_detection_of_brain_tumor' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/JeissonParra12/Class_project_Early_detection_of_brain_tumor.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a2bae7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting Complete Brain Tumor Detection Pipeline...\n",
      "‚ö†Ô∏è CLM model not found or error loading: name 'CorrelationLearningMechanism' is not defined\n",
      "Please run feature extraction first or continue with detection from scratch\n",
      "Using device: cpu\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'BrainTumorDataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-1568858248.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    762\u001b[0m     \u001b[0;31m# Run the complete detection pipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 763\u001b[0;31m     \u001b[0mrun_complete_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipython-input-1568858248.py\u001b[0m in \u001b[0;36mrun_complete_pipeline\u001b[0;34m()\u001b[0m\n\u001b[1;32m    757\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m     \u001b[0;31m# Now run detection and classification\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 759\u001b[0;31m     \u001b[0mmain_detection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    760\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipython-input-1568858248.py\u001b[0m in \u001b[0;36mmain_detection\u001b[0;34m()\u001b[0m\n\u001b[1;32m    639\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m     \u001b[0;31m# Create datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 641\u001b[0;31m     \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBrainTumorDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    642\u001b[0m     \u001b[0mval_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBrainTumorDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"val\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m     \u001b[0mtest_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBrainTumorDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"test\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'BrainTumorDataset' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple, Optional, Union\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ============================================================================\n",
    "# REGION PROPOSAL NETWORK (RPN) FOR TUMOR DETECTION - FIXED VERSION\n",
    "# ============================================================================\n",
    "\n",
    "class LightweightRPN(nn.Module):\n",
    "    \"\"\"\n",
    "    Lightweight Region Proposal Network for detecting potential tumor regions\n",
    "    Optimized for high recall to ensure small lesions are not overlooked\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_channels: int = 4, num_anchors: int = 9):\n",
    "        super(LightweightRPN, self).__init__()\n",
    "        \n",
    "        self.input_channels = input_channels\n",
    "        self.num_anchors = num_anchors\n",
    "        \n",
    "        # Feature extraction backbone (lightweight)\n",
    "        self.backbone = nn.Sequential(\n",
    "            # First conv block\n",
    "            nn.Conv2d(input_channels, 64, 3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            \n",
    "            # Second conv block\n",
    "            nn.Conv2d(64, 128, 3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            \n",
    "            # Third conv block\n",
    "            nn.Conv2d(128, 256, 3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        \n",
    "        # Classification head (tumor vs background)\n",
    "        self.cls_head = nn.Sequential(\n",
    "            nn.Conv2d(256, 256, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, num_anchors * 2, 1)  # 2 scores per anchor (tumor, background)\n",
    "        )\n",
    "        \n",
    "        # Regression head (bounding box adjustments)\n",
    "        self.reg_head = nn.Sequential(\n",
    "            nn.Conv2d(256, 256, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, num_anchors * 4, 1)  # 4 coordinates per anchor\n",
    "        )\n",
    "        \n",
    "        # Initialize anchors for different scales and aspect ratios\n",
    "        self.anchor_scales = [32, 64, 128]  # Different sizes for small/medium/large tumors\n",
    "        self.anchor_ratios = [0.5, 1.0, 2.0]  # Different aspect ratios\n",
    "        \n",
    "        # We'll generate anchors dynamically based on feature map size\n",
    "        self.anchors = None\n",
    "        \n",
    "    def generate_anchors(self, feature_map_size: Tuple[int, int], device: torch.device) -> torch.Tensor:\n",
    "        \"\"\"Generate anchor boxes for a given feature map size\"\"\"\n",
    "        if self.anchors is not None:\n",
    "            return self.anchors\n",
    "            \n",
    "        # Calculate the stride based on input size (224) and feature map size\n",
    "        stride_h = 224 / feature_map_size[0]\n",
    "        stride_w = 224 / feature_map_size[1]\n",
    "        \n",
    "        anchors = []\n",
    "        \n",
    "        # Generate anchors for each position in the feature map\n",
    "        for i in range(feature_map_size[0]):\n",
    "            for j in range(feature_map_size[1]):\n",
    "                center_y = (i + 0.5) * stride_h\n",
    "                center_x = (j + 0.5) * stride_w\n",
    "                \n",
    "                for scale in self.anchor_scales:\n",
    "                    for ratio in self.anchor_ratios:\n",
    "                        w = scale * np.sqrt(ratio)\n",
    "                        h = scale / np.sqrt(ratio)\n",
    "                        \n",
    "                        x1 = center_x - w / 2\n",
    "                        y1 = center_y - h / 2\n",
    "                        x2 = center_x + w / 2\n",
    "                        y2 = center_y + h / 2\n",
    "                        \n",
    "                        anchors.append([x1, y1, x2, y2])\n",
    "        \n",
    "        anchors = torch.tensor(anchors, dtype=torch.float32, device=device)\n",
    "        self.anchors = anchors\n",
    "        return anchors\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Forward pass through RPN\n",
    "        Returns: tuple of (classification_scores, bounding_box_regressions)\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # Extract features\n",
    "        features = self.backbone(x)  # Shape: (batch_size, 256, H, W)\n",
    "        \n",
    "        # Get feature map size\n",
    "        feature_map_size = (features.size(2), features.size(3))\n",
    "        \n",
    "        # Generate anchors for this feature map size\n",
    "        anchors = self.generate_anchors(feature_map_size, x.device)\n",
    "        total_anchors = anchors.size(0)\n",
    "        \n",
    "        # Get classification scores and bounding box regressions\n",
    "        cls_scores = self.cls_head(features)\n",
    "        reg_preds = self.reg_head(features)\n",
    "        \n",
    "        # Reshape outputs\n",
    "        cls_scores = cls_scores.permute(0, 2, 3, 1).contiguous().view(batch_size, -1, 2)\n",
    "        reg_preds = reg_preds.permute(0, 2, 3, 1).contiguous().view(batch_size, -1, 4)\n",
    "        \n",
    "        # Store anchors for later use\n",
    "        self.current_anchors = anchors\n",
    "        \n",
    "        return cls_scores, reg_preds\n",
    "\n",
    "# ============================================================================\n",
    "# SIMPLIFIED TWO-STAGE DETECTION AND CLASSIFICATION SYSTEM - FIXED VERSION\n",
    "# ============================================================================\n",
    "\n",
    "class BrainTumorDetectionSystem(nn.Module):\n",
    "    \"\"\"\n",
    "    Simplified two-stage brain tumor detection and classification system\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_channels: int = 4, num_classes: int = 2):\n",
    "        super(BrainTumorDetectionSystem, self).__init__()\n",
    "        \n",
    "        # Stage 1: Region Proposal Network\n",
    "        self.rpn = LightweightRPN(input_channels)\n",
    "        \n",
    "        # Stage 2: CLM-based classification\n",
    "        self.clm_classifier = CorrelationLearningMechanism(input_channels, num_classes)\n",
    "        \n",
    "        # Get the actual feature dimension from CLM\n",
    "        self.clm_feature_dim = self._get_clm_feature_dim()\n",
    "        \n",
    "        print(f\"CLM feature dimension: {self.clm_feature_dim}\")\n",
    "        \n",
    "        # Fusion layer to combine features - FIXED DIMENSIONS\n",
    "        self.fusion_layer = nn.Sequential(\n",
    "            nn.Linear(self.clm_feature_dim + 2, 128),  # CLM features + RPN scores\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "        \n",
    "    def _get_clm_feature_dim(self) -> int:\n",
    "        \"\"\"Get the actual feature dimension from CLM classifier\"\"\"\n",
    "        # Create a dummy input to get the feature dimension\n",
    "        dummy_input = torch.randn(1, 4, 224, 224)\n",
    "        with torch.no_grad():\n",
    "            features, _ = self.clm_classifier(dummy_input)\n",
    "            if features.dim() == 4:\n",
    "                features = F.adaptive_avg_pool2d(features, (1, 1))\n",
    "                features = features.view(1, -1)\n",
    "            return features.size(1)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Fixed forward pass with proper dimension handling\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # Get RPN predictions\n",
    "        rpn_cls, rpn_reg = self.rpn(x)\n",
    "        \n",
    "        # Get RPN image-level scores\n",
    "        rpn_scores = F.softmax(rpn_cls, dim=2)[:, :, 1]  # Tumor probability for each anchor\n",
    "        rpn_max_scores, _ = rpn_scores.max(dim=1)  # Max score per image\n",
    "        \n",
    "        # Create RPN feature representation (tumor score and background score)\n",
    "        rpn_feature = torch.stack([rpn_max_scores, 1 - rpn_max_scores], dim=1)  # Shape: (batch_size, 2)\n",
    "        \n",
    "        # Get CLM features\n",
    "        clm_features, clm_classification = self.clm_classifier(x)\n",
    "        \n",
    "        # Process CLM features to ensure correct dimensions\n",
    "        if clm_features.dim() == 4:\n",
    "            clm_features = F.adaptive_avg_pool2d(clm_features, (1, 1))\n",
    "            clm_features = clm_features.view(batch_size, -1)\n",
    "        \n",
    "        # Ensure CLM features have the expected dimension\n",
    "        if clm_features.size(1) != self.clm_feature_dim:\n",
    "            # Handle dimension mismatch by projection\n",
    "            if clm_features.size(1) < self.clm_feature_dim:\n",
    "                # Pad if smaller\n",
    "                padding = torch.zeros(batch_size, self.clm_feature_dim - clm_features.size(1), \n",
    "                                    device=x.device)\n",
    "                clm_features = torch.cat([clm_features, padding], dim=1)\n",
    "            else:\n",
    "                # Truncate if larger\n",
    "                clm_features = clm_features[:, :self.clm_feature_dim]\n",
    "        \n",
    "        # Combine features\n",
    "        combined_features = torch.cat([clm_features, rpn_feature], dim=1)\n",
    "        \n",
    "        # Final classification\n",
    "        final_classification = self.fusion_layer(combined_features)\n",
    "        \n",
    "        return final_classification, rpn_cls, rpn_reg, clm_features\n",
    "    \n",
    "    def detect_regions(self, x: torch.Tensor, threshold: float = 0.3):\n",
    "        \"\"\"\n",
    "        Separate method for region detection - FIXED VERSION\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "        rpn_cls, rpn_reg = self.rpn(x)\n",
    "        \n",
    "        proposals_list = []\n",
    "        confidence_list = []\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            # Get scores for this image\n",
    "            image_cls = rpn_cls[i]  # Shape: (num_anchors, 2)\n",
    "            image_reg = rpn_reg[i]  # Shape: (num_anchors, 4)\n",
    "            \n",
    "            # Convert to probabilities\n",
    "            probs = F.softmax(image_cls, dim=1)[:, 1]  # Tumor probability\n",
    "            \n",
    "            # Get anchors for this forward pass\n",
    "            anchors = self.rpn.current_anchors\n",
    "            \n",
    "            # Ensure anchors and probabilities have the same number of elements\n",
    "            if anchors.size(0) != probs.size(0):\n",
    "                # This should not happen, but if it does, use the minimum\n",
    "                min_size = min(anchors.size(0), probs.size(0))\n",
    "                anchors = anchors[:min_size]\n",
    "                probs = probs[:min_size]\n",
    "                image_reg = image_reg[:min_size]\n",
    "            \n",
    "            # Simple threshold-based proposal generation\n",
    "            keep_mask = probs > threshold\n",
    "            \n",
    "            if keep_mask.sum() > 0:\n",
    "                kept_anchors = anchors[keep_mask]\n",
    "                kept_reg = image_reg[keep_mask]\n",
    "                kept_probs = probs[keep_mask]\n",
    "                \n",
    "                # Apply regression adjustments\n",
    "                adjusted_proposals = self._apply_regression(kept_anchors, kept_reg)\n",
    "                proposals_list.append(adjusted_proposals)\n",
    "                confidence_list.append(kept_probs)\n",
    "            else:\n",
    "                # No proposals - return empty tensors\n",
    "                proposals_list.append(torch.empty(0, 4, device=x.device))\n",
    "                confidence_list.append(torch.empty(0, device=x.device))\n",
    "        \n",
    "        return proposals_list, confidence_list\n",
    "    \n",
    "    def _apply_regression(self, anchors: torch.Tensor, regressions: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Apply bounding box regression adjustments to anchors\"\"\"\n",
    "        # Convert from [x1, y1, x2, y2] to [center_x, center_y, width, height]\n",
    "        widths = anchors[:, 2] - anchors[:, 0]\n",
    "        heights = anchors[:, 3] - anchors[:, 1]\n",
    "        center_x = anchors[:, 0] + 0.5 * widths\n",
    "        center_y = anchors[:, 1] + 0.5 * heights\n",
    "        \n",
    "        # Apply regression (dx, dy, dw, dh)\n",
    "        dx = regressions[:, 0]\n",
    "        dy = regressions[:, 1]\n",
    "        dw = regressions[:, 2]\n",
    "        dh = regressions[:, 3]\n",
    "        \n",
    "        pred_center_x = center_x + dx * widths\n",
    "        pred_center_y = center_y + dy * heights\n",
    "        pred_width = widths * torch.exp(dw)\n",
    "        pred_height = heights * torch.exp(dh)\n",
    "        \n",
    "        # Convert back to [x1, y1, x2, y2] format\n",
    "        pred_x1 = pred_center_x - 0.5 * pred_width\n",
    "        pred_y1 = pred_center_y - 0.5 * pred_height\n",
    "        pred_x2 = pred_center_x + 0.5 * pred_width\n",
    "        pred_y2 = pred_center_y + 0.5 * pred_height\n",
    "        \n",
    "        # Clip to image boundaries\n",
    "        pred_x1 = torch.clamp(pred_x1, 0, 223)\n",
    "        pred_y1 = torch.clamp(pred_y1, 0, 223)\n",
    "        pred_x2 = torch.clamp(pred_x2, 0, 223)\n",
    "        pred_y2 = torch.clamp(pred_y2, 0, 223)\n",
    "        \n",
    "        return torch.stack([pred_x1, pred_y1, pred_x2, pred_y2], dim=1)\n",
    "\n",
    "# ============================================================================\n",
    "# SIZE-AWARE LOSS FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "class SizeAwareLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Size-aware loss function that gives higher weight to small tumor examples\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, alpha: float = 0.25, gamma: float = 2.0, size_weights: Dict[str, float] = None):\n",
    "        super(SizeAwareLoss, self).__init__()\n",
    "        \n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        # Default weights for different tumor sizes (small tumors get higher weight)\n",
    "        if size_weights is None:\n",
    "            size_weights = {\n",
    "                'small': 3.0,    # Highest weight for small tumors\n",
    "                'medium': 1.5,   # Medium weight for medium tumors  \n",
    "                'large': 1.0     # Base weight for large tumors\n",
    "            }\n",
    "        self.size_weights = size_weights\n",
    "        \n",
    "    def forward(self, predictions: torch.Tensor, targets: torch.Tensor, \n",
    "                tumor_sizes: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute size-aware loss\n",
    "        \"\"\"\n",
    "        # Standard cross entropy loss\n",
    "        ce_loss = F.cross_entropy(predictions, targets, reduction='none')\n",
    "        \n",
    "        # Apply focal loss component to focus on hard examples\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss\n",
    "        \n",
    "        # Apply size-based weighting if tumor sizes are provided\n",
    "        if tumor_sizes is not None:\n",
    "            size_weights = self._get_size_weights(tumor_sizes).to(predictions.device)\n",
    "            focal_loss = focal_loss * size_weights\n",
    "        \n",
    "        return focal_loss.mean()\n",
    "    \n",
    "    def _get_size_weights(self, tumor_sizes: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Convert tumor sizes to appropriate weights\"\"\"\n",
    "        weights = torch.ones_like(tumor_sizes, dtype=torch.float32)\n",
    "        \n",
    "        # Small tumors (area < 500 pixels)\n",
    "        small_mask = tumor_sizes < 500\n",
    "        weights[small_mask] = self.size_weights['small']\n",
    "        \n",
    "        # Medium tumors (500 <= area < 2000 pixels)\n",
    "        medium_mask = (tumor_sizes >= 500) & (tumor_sizes < 2000)\n",
    "        weights[medium_mask] = self.size_weights['medium']\n",
    "        \n",
    "        return weights\n",
    "\n",
    "# ============================================================================\n",
    "# DETECTION TRAINER\n",
    "# ============================================================================\n",
    "\n",
    "class DetectionTrainer:\n",
    "    \"\"\"\n",
    "    Trainer for the detection and classification system\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model: nn.Module, device: torch.device):\n",
    "        self.model = model.to(device)\n",
    "        self.device = device\n",
    "        \n",
    "        # Optimizer - only optimize unfrozen parameters\n",
    "        trainable_params = [p for p in model.parameters() if p.requires_grad]\n",
    "        self.optimizer = optim.AdamW(trainable_params, lr=1e-4, weight_decay=1e-4)\n",
    "        \n",
    "        # Loss function\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        # Tracking\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.train_accuracies = []\n",
    "        self.val_accuracies = []\n",
    "        \n",
    "    def train_epoch(self, train_loader: DataLoader) -> Tuple[float, float]:\n",
    "        \"\"\"Train for one epoch\"\"\"\n",
    "        self.model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for batch_idx, (data, targets) in enumerate(train_loader):\n",
    "            data, targets = data.to(self.device), targets.to(self.device)\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            final_classification, rpn_cls, rpn_reg, clm_features = self.model(data)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = self.criterion(final_classification, targets)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = final_classification.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "            \n",
    "            if batch_idx % 20 == 0:\n",
    "                accuracy = 100. * correct / total if total > 0 else 0\n",
    "                print(f'Batch {batch_idx}, Loss: {loss.item():.4f}, Acc: {accuracy:.2f}%')\n",
    "        \n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        epoch_accuracy = 100. * correct / total if total > 0 else 0\n",
    "        \n",
    "        return epoch_loss, epoch_accuracy\n",
    "    \n",
    "    def validate_epoch(self, val_loader: DataLoader) -> Tuple[float, float]:\n",
    "        \"\"\"Validate for one epoch\"\"\"\n",
    "        self.model.eval()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for data, targets in val_loader:\n",
    "                data, targets = data.to(self.device), targets.to(self.device)\n",
    "                \n",
    "                # Forward pass\n",
    "                final_classification, rpn_cls, rpn_reg, clm_features = self.model(data)\n",
    "                \n",
    "                # Compute loss and accuracy\n",
    "                loss = self.criterion(final_classification, targets)\n",
    "                \n",
    "                running_loss += loss.item()\n",
    "                _, predicted = final_classification.max(1)\n",
    "                total += targets.size(0)\n",
    "                correct += predicted.eq(targets).sum().item()\n",
    "        \n",
    "        epoch_loss = running_loss / len(val_loader) if len(val_loader) > 0 else 0\n",
    "        epoch_accuracy = 100. * correct / total if total > 0 else 0\n",
    "        \n",
    "        return epoch_loss, epoch_accuracy\n",
    "    \n",
    "    def train(self, train_loader: DataLoader, val_loader: DataLoader, epochs: int = 15):\n",
    "        \"\"\"Complete training procedure\"\"\"\n",
    "        print(\"Starting Two-Stage Detection Training...\")\n",
    "        \n",
    "        best_accuracy = 0.0\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Training\n",
    "            train_loss, train_acc = self.train_epoch(train_loader)\n",
    "            self.train_losses.append(train_loss)\n",
    "            self.train_accuracies.append(train_acc)\n",
    "            \n",
    "            # Validation\n",
    "            val_loss, val_acc = self.validate_epoch(val_loader)\n",
    "            self.val_losses.append(val_loss)\n",
    "            self.val_accuracies.append(val_acc)\n",
    "            \n",
    "            print(f'Epoch {epoch+1}/{epochs}:')\n",
    "            print(f'  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%')\n",
    "            print(f'  Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')\n",
    "            \n",
    "            # Save best model\n",
    "            if val_acc > best_accuracy:\n",
    "                best_accuracy = val_acc\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': self.model.state_dict(),\n",
    "                    'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "                    'val_accuracy': val_acc,\n",
    "                }, 'best_detection_model.pth')\n",
    "                print(f'  New best model saved with validation accuracy: {val_acc:.2f}%')\n",
    "            \n",
    "            print('-' * 50)\n",
    "\n",
    "# ============================================================================\n",
    "# VISUALIZATION AND EVALUATION TOOLS - FIXED VERSION\n",
    "# ============================================================================\n",
    "\n",
    "def visualize_detections(model: nn.Module, dataloader: DataLoader, device: torch.device, num_examples: int = 5):\n",
    "    \"\"\"Visualize model detections on sample images - FIXED VERSION\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    fig, axes = plt.subplots(num_examples, 2, figsize=(15, 3*num_examples))\n",
    "    if num_examples == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    examples_processed = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, targets in dataloader:\n",
    "            if examples_processed >= num_examples:\n",
    "                break\n",
    "                \n",
    "            data, targets = data.to(device), targets.to(device)\n",
    "            \n",
    "            # Use detection method\n",
    "            proposals, confidences = model.detect_regions(data, threshold=0.3)\n",
    "            \n",
    "            for i in range(data.size(0)):\n",
    "                if examples_processed >= num_examples:\n",
    "                    break\n",
    "                    \n",
    "                # Get original image (first channel)\n",
    "                original_image = data[i, 0].cpu().numpy()\n",
    "                \n",
    "                # Plot original image\n",
    "                axes[examples_processed, 0].imshow(original_image, cmap='gray')\n",
    "                axes[examples_processed, 0].set_title(f'Original (Label: {\"Tumor\" if targets[i].item() == 1 else \"Normal\"})')\n",
    "                axes[examples_processed, 0].axis('off')\n",
    "                \n",
    "                # Plot detections\n",
    "                axes[examples_processed, 1].imshow(original_image, cmap='gray')\n",
    "                image_proposals = proposals[i]\n",
    "                image_confidences = confidences[i]\n",
    "                \n",
    "                if len(image_proposals) > 0:\n",
    "                    for proposal, confidence in zip(image_proposals, image_confidences):\n",
    "                        x1, y1, x2, y2 = proposal.cpu().numpy()\n",
    "                        \n",
    "                        # Ensure valid coordinates\n",
    "                        x1 = max(0, min(x1, 223))\n",
    "                        y1 = max(0, min(y1, 223))\n",
    "                        x2 = max(0, min(x2, 223))\n",
    "                        y2 = max(0, min(y2, 223))\n",
    "                        \n",
    "                        if x2 > x1 and y2 > y1:  # Valid proposal\n",
    "                            rect = patches.Rectangle((x1, y1), x2-x1, y2-y1, \n",
    "                                                   linewidth=2, edgecolor='r', facecolor='none')\n",
    "                            axes[examples_processed, 1].add_patch(rect)\n",
    "                            \n",
    "                            # Add confidence text\n",
    "                            axes[examples_processed, 1].text(x1, max(0, y1-5), f'{confidence:.2f}', \n",
    "                                             color='red', fontsize=8, weight='bold')\n",
    "                \n",
    "                axes[examples_processed, 1].set_title(f'Detections ({len(image_proposals)} regions)')\n",
    "                axes[examples_processed, 1].axis('off')\n",
    "                \n",
    "                examples_processed += 1\n",
    "    \n",
    "    # Hide unused subplots\n",
    "    for i in range(examples_processed, num_examples):\n",
    "        axes[i, 0].axis('off')\n",
    "        axes[i, 1].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('detection_visualizations.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "def evaluate_detection_performance(model: nn.Module, dataloader: DataLoader, device: torch.device):\n",
    "    \"\"\"Evaluate detection performance\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    detection_counts = []  # Number of regions detected per image\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, targets in dataloader:\n",
    "            data, targets = data.to(device), targets.to(device)\n",
    "            \n",
    "            # Get image-level predictions\n",
    "            final_classification, _, _, _ = model(data)\n",
    "            \n",
    "            # Get detection counts\n",
    "            proposals, _ = model.detect_regions(data, threshold=0.3)\n",
    "            \n",
    "            for i in range(data.size(0)):\n",
    "                all_predictions.append(F.softmax(final_classification[i], dim=0)[1].item())\n",
    "                all_targets.append(targets[i].item())\n",
    "                detection_counts.append(len(proposals[i]))\n",
    "    \n",
    "    # Convert to numpy\n",
    "    predictions = np.array(all_predictions)\n",
    "    targets = np.array(all_targets)\n",
    "    detection_counts = np.array(detection_counts)\n",
    "    \n",
    "    # Compute metrics\n",
    "    results = {}\n",
    "    thresholds = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        pred_labels = (predictions >= threshold).astype(int)\n",
    "        accuracy = (pred_labels == targets).mean()\n",
    "        recall = (pred_labels[targets == 1] == 1).mean() if (targets == 1).sum() > 0 else 0\n",
    "        precision = (targets[pred_labels == 1] == 1).mean() if (pred_labels == 1).sum() > 0 else 0\n",
    "        \n",
    "        results[threshold] = {\n",
    "            'accuracy': accuracy,\n",
    "            'recall': recall,\n",
    "            'precision': precision\n",
    "        }\n",
    "    \n",
    "    # Plot precision-recall curve\n",
    "    precision_vals, recall_vals, _ = precision_recall_curve(targets, predictions)\n",
    "    ap = average_precision_score(targets, predictions)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(recall_vals, precision_vals, linewidth=2)\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title(f'Detection Precision-Recall Curve (AP: {ap:.4f})')\n",
    "    plt.grid(True)\n",
    "    plt.savefig('detection_precision_recall.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Analyze detection counts\n",
    "    if len(targets[targets == 1]) > 0:\n",
    "        tumor_detections = detection_counts[targets == 1]\n",
    "        print(f\"Average detections in tumor images: {tumor_detections.mean():.2f} ¬± {tumor_detections.std():.2f}\")\n",
    "    \n",
    "    if len(targets[targets == 0]) > 0:\n",
    "        normal_detections = detection_counts[targets == 0]\n",
    "        print(f\"Average detections in normal images: {normal_detections.mean():.2f} ¬± {normal_detections.std():.2f}\")\n",
    "    \n",
    "    return results, ap\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN DETECTION PIPELINE\n",
    "# ============================================================================\n",
    "\n",
    "def main_detection():\n",
    "    \"\"\"Main function for detection and classification pipeline\"\"\"\n",
    "    # Configuration\n",
    "    DATA_DIR = \"/content/Class_project_Early_detection_of_brain_tumor/CT_enhanced\"\n",
    "    BATCH_SIZE = 8\n",
    "    EPOCHS = 15\n",
    "    \n",
    "    # Set device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = BrainTumorDataset(DATA_DIR, split=\"train\")\n",
    "    val_dataset = BrainTumorDataset(DATA_DIR, split=\"val\")\n",
    "    test_dataset = BrainTumorDataset(DATA_DIR, split=\"test\")\n",
    "    \n",
    "    print(f\"Training samples: {len(train_dataset)}\")\n",
    "    print(f\"Validation samples: {len(val_dataset)}\")\n",
    "    print(f\"Test samples: {len(test_dataset)}\")\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "    \n",
    "    # Initialize detection system\n",
    "    detection_model = BrainTumorDetectionSystem(input_channels=4, num_classes=2)\n",
    "    \n",
    "    # Load pre-trained CLM weights\n",
    "    try:\n",
    "        clm_checkpoint = torch.load('best_clm_model.pth', map_location=device)\n",
    "        # Transfer CLM weights to detection system\n",
    "        detection_model.clm_classifier.load_state_dict(clm_checkpoint['model_state_dict'])\n",
    "        print(\"‚úÖ Loaded pre-trained CLM weights\")\n",
    "        \n",
    "        # Freeze CLM initially for stable training\n",
    "        for param in detection_model.clm_classifier.parameters():\n",
    "            param.requires_grad = False\n",
    "        print(\"‚úÖ Frozen CLM weights for initial training\")\n",
    "        \n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"‚ö†Ô∏è No pre-trained CLM found: {e}\")\n",
    "        print(\"Training from scratch...\")\n",
    "    \n",
    "    # Initialize trainer\n",
    "    trainer = DetectionTrainer(detection_model, device)\n",
    "    \n",
    "    # Train the detection system\n",
    "    trainer.train(train_loader, val_loader, epochs=EPOCHS)\n",
    "    \n",
    "    # Unfreeze CLM for fine-tuning if it was frozen\n",
    "    try:\n",
    "        for param in detection_model.clm_classifier.parameters():\n",
    "            param.requires_grad = True\n",
    "        print(\"‚úÖ Unfrozen CLM weights for fine-tuning\")\n",
    "        \n",
    "        # Reinitialize optimizer with all parameters\n",
    "        trainer.optimizer = optim.AdamW(detection_model.parameters(), lr=5e-5, weight_decay=1e-4)\n",
    "        print(\"Fine-tuning with unfrozen CLM...\")\n",
    "        trainer.train(train_loader, val_loader, epochs=5)\n",
    "    except Exception as e:\n",
    "        print(f\"Fine-tuning skipped: {e}\")\n",
    "    \n",
    "    # Visualize detections\n",
    "    print(\"Visualizing detections...\")\n",
    "    visualize_detections(detection_model, val_loader, device, num_examples=5)\n",
    "    \n",
    "    # Evaluate detection performance\n",
    "    print(\"Evaluating detection performance...\")\n",
    "    results, ap = evaluate_detection_performance(detection_model, test_loader, device)\n",
    "    \n",
    "    print(\"\\nüìä Detection Results:\")\n",
    "    for threshold, metrics in results.items():\n",
    "        print(f\"Threshold {threshold}: \"\n",
    "              f\"Accuracy={metrics['accuracy']:.4f}, \"\n",
    "              f\"Recall={metrics['recall']:.4f}, \"\n",
    "              f\"Precision={metrics['precision']:.4f}\")\n",
    "    print(f\"Average Precision: {ap:.4f}\")\n",
    "    \n",
    "    # Plot training history\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(trainer.train_losses, label='Training Loss')\n",
    "    plt.plot(trainer.val_losses, label='Validation Loss')\n",
    "    plt.title('Detection Training Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(trainer.train_accuracies, label='Training Accuracy')\n",
    "    plt.plot(trainer.val_accuracies, label='Validation Accuracy')\n",
    "    plt.title('Detection Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('detection_training_history.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n‚úÖ Two-Stage Detection and Classification completed successfully!\")\n",
    "    print(\"üìä Model saved as: best_detection_model.pth\")\n",
    "    print(\"üñºÔ∏è Detection visualizations saved as: detection_visualizations.png\")\n",
    "    print(\"üìà Evaluation results saved as: detection_precision_recall.png\")\n",
    "\n",
    "# ============================================================================\n",
    "# INTEGRATION WITH EXISTING PIPELINE\n",
    "# ============================================================================\n",
    "\n",
    "def run_complete_pipeline():\n",
    "    \"\"\"Run the complete pipeline from feature extraction to detection\"\"\"\n",
    "    print(\"üöÄ Starting Complete Brain Tumor Detection Pipeline...\")\n",
    "    \n",
    "    # First, ensure feature extraction is complete\n",
    "    try:\n",
    "        # Test if CLM model exists\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        clm_model = CorrelationLearningMechanism(input_channels=4, num_classes=2)\n",
    "        clm_checkpoint = torch.load('best_clm_model.pth', map_location=device)\n",
    "        clm_model.load_state_dict(clm_checkpoint['model_state_dict'])\n",
    "        print(\"‚úÖ Loaded trained CLM model\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è CLM model not found or error loading: {e}\")\n",
    "        print(\"Please run feature extraction first or continue with detection from scratch\")\n",
    "    \n",
    "    # Now run detection and classification\n",
    "    main_detection()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run the complete detection pipeline\n",
    "    run_complete_pipeline()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
