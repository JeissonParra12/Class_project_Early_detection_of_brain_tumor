{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c098e20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'Class_project_Early_detection_of_brain_tumor'...\n",
      "remote: Enumerating objects: 13907, done.\u001b[K\n",
      "remote: Counting objects: 100% (10/10), done.\u001b[K\n",
      "remote: Compressing objects: 100% (8/8), done.\u001b[K\n",
      "remote: Total 13907 (delta 3), reused 6 (delta 2), pack-reused 13897 (from 3)\u001b[K\n",
      "Receiving objects: 100% (13907/13907), 774.01 MiB | 40.65 MiB/s, done.\n",
      "Resolving deltas: 100% (3856/3856), done.\n",
      "Updating files: 100% (16383/16383), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/JeissonParra12/Class_project_Early_detection_of_brain_tumor.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7622babe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Training samples: 3861\n",
      "Validation samples: 1028\n",
      "Test samples: 987\n",
      "Total output channels from conv filters: 160\n",
      "Model initialized with 1,582,365 parameters\n",
      "Testing forward pass with a single batch...\n",
      "Forward pass successful! Features shape: torch.Size([16, 128]), Outputs shape: torch.Size([16, 2])\n",
      "Starting Feature Extraction and Correlation Learning Training...\n",
      "Batch 0, Loss: 0.7181\n",
      "Batch 50, Loss: 0.6041\n",
      "Batch 100, Loss: 0.5229\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-2402292321.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    545\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipython-input-2402292321.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m     \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m     \u001b[0;31m# Plot training history\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipython-input-2402292321.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, train_loader, val_loader, epochs)\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m             \u001b[0;31m# Training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 386\u001b[0;31m             \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    387\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_accuracies\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipython-input-2402292321.py\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(self, train_loader)\u001b[0m\n\u001b[1;32m    341\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 343\u001b[0;31m             \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    344\u001b[0m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m             \u001b[0mtotal\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from scipy.stats import pearsonr\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class CorrelationLearningMechanism(nn.Module):\n",
    "    \"\"\"\n",
    "    Correlation Learning Mechanism (CLM) inspired by Wo≈∫niak et al. (2023)\n",
    "    Dynamically filters convolutional layer combinations and evaluates feature correlations\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_channels: int = 4, num_classes: int = 2):\n",
    "        super(CorrelationLearningMechanism, self).__init__()\n",
    "        \n",
    "        self.input_channels = input_channels\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # Multi-scale feature extraction branches\n",
    "        self.branch_configs = self._create_branch_configurations()\n",
    "        \n",
    "        # Dynamic convolutional filter banks\n",
    "        self.conv_filters = nn.ModuleDict()\n",
    "        self._initialize_conv_filters()\n",
    "        \n",
    "        # Calculate the actual output channels from conv_filters\n",
    "        total_output_channels = 0\n",
    "        for name in self.conv_filters:\n",
    "            if 'standard_3x3' in name:\n",
    "                total_output_channels += 32\n",
    "            elif 'standard_5x5' in name:\n",
    "                total_output_channels += 32\n",
    "            elif 'depthwise_3x3' in name:\n",
    "                total_output_channels += 32\n",
    "            elif 'dilated_3x3' in name:\n",
    "                total_output_channels += 32\n",
    "            elif 'asymmetric_1x3' in name:\n",
    "                total_output_channels += 32\n",
    "        \n",
    "        print(f\"Total output channels from conv filters: {total_output_channels}\")\n",
    "        \n",
    "        # Correlation learning components - FIXED CHANNEL DIMENSIONS\n",
    "        self.correlation_net = CorrelationNetwork(total_output_channels, 256)\n",
    "        self.feature_selector = FastCorrelationFeatureSelector(256, 128)\n",
    "        \n",
    "        # Classification head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(64, num_classes)\n",
    "        )\n",
    "        \n",
    "    def _create_branch_configurations(self) -> List[Dict]:\n",
    "        \"\"\"Create different branch configurations for dynamic filtering\"\"\"\n",
    "        configs = [\n",
    "            # Branch 1: Standard convolution\n",
    "            {'filters': 32, 'kernel_size': 3, 'pool_type': 'max', 'activation': 'relu'},\n",
    "            # Branch 2: Depth-wise separable convolution\n",
    "            {'filters': 64, 'kernel_size': 5, 'pool_type': 'avg', 'activation': 'relu'},\n",
    "            # Branch 3: Dilated convolution for larger receptive field\n",
    "            {'filters': 32, 'kernel_size': 3, 'dilation': 2, 'pool_type': 'max', 'activation': 'leaky_relu'},\n",
    "            # Branch 4: Asymmetric convolution\n",
    "            {'filters': 64, 'kernel_size': (1, 3), 'pool_type': 'avg', 'activation': 'relu'},\n",
    "        ]\n",
    "        return configs\n",
    "    \n",
    "    def _initialize_conv_filters(self):\n",
    "        \"\"\"Initialize different convolutional filter types\"\"\"\n",
    "        # Standard convolutional layers\n",
    "        self.conv_filters['standard_3x3'] = nn.Conv2d(self.input_channels, 32, 3, padding=1)\n",
    "        self.conv_filters['standard_5x5'] = nn.Conv2d(self.input_channels, 32, 5, padding=2)\n",
    "        \n",
    "        # Depth-wise separable convolutions\n",
    "        self.conv_filters['depthwise_3x3'] = nn.Sequential(\n",
    "            nn.Conv2d(self.input_channels, self.input_channels, 3, padding=1, groups=self.input_channels),\n",
    "            nn.Conv2d(self.input_channels, 32, 1)\n",
    "        )\n",
    "        \n",
    "        # Dilated convolutions\n",
    "        self.conv_filters['dilated_3x3'] = nn.Conv2d(self.input_channels, 32, 3, padding=2, dilation=2)\n",
    "        \n",
    "        # Asymmetric convolutions\n",
    "        self.conv_filters['asymmetric_1x3'] = nn.Sequential(\n",
    "            nn.Conv2d(self.input_channels, 32, (1, 3), padding=(0, 1)),\n",
    "            nn.Conv2d(32, 32, (3, 1), padding=(1, 0))\n",
    "        )\n",
    "    \n",
    "    def _apply_dynamic_pooling(self, x: torch.Tensor, pool_type: str) -> torch.Tensor:\n",
    "        \"\"\"Apply dynamic pooling operations\"\"\"\n",
    "        if pool_type == 'max':\n",
    "            return F.adaptive_max_pool2d(x, (x.size(2)//2, x.size(3)//2))\n",
    "        elif pool_type == 'avg':\n",
    "            return F.adaptive_avg_pool2d(x, (x.size(2)//2, x.size(3)//2))\n",
    "        elif pool_type == 'mixed':\n",
    "            max_pool = F.adaptive_max_pool2d(x, (x.size(2)//2, x.size(3)//2))\n",
    "            avg_pool = F.adaptive_avg_pool2d(x, (x.size(2)//2, x.size(3)//2))\n",
    "            return (max_pool + avg_pool) / 2\n",
    "        else:\n",
    "            return x\n",
    "    \n",
    "    def _apply_activation(self, x: torch.Tensor, activation: str) -> torch.Tensor:\n",
    "        \"\"\"Apply dynamic activation functions\"\"\"\n",
    "        if activation == 'relu':\n",
    "            return F.relu(x)\n",
    "        elif activation == 'leaky_relu':\n",
    "            return F.leaky_relu(x, 0.1)\n",
    "        elif activation == 'elu':\n",
    "            return F.elu(x)\n",
    "        elif activation == 'selu':\n",
    "            return F.selu(x)\n",
    "        else:\n",
    "            return x\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Forward pass through CLM\n",
    "        Returns: tuple of (features, classification_logits)\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # Extract features using multiple filter configurations\n",
    "        branch_outputs = []\n",
    "        \n",
    "        for name, filter_module in self.conv_filters.items():\n",
    "            # Apply convolutional filter\n",
    "            filtered = filter_module(x)\n",
    "            \n",
    "            # Apply pooling based on filter type\n",
    "            if 'dilated' in name:\n",
    "                pooled = self._apply_dynamic_pooling(filtered, 'max')\n",
    "            elif 'depthwise' in name:\n",
    "                pooled = self._apply_dynamic_pooling(filtered, 'avg')\n",
    "            else:\n",
    "                pooled = self._apply_dynamic_pooling(filtered, 'mixed')\n",
    "            \n",
    "            # Apply activation\n",
    "            if 'leaky' in name:\n",
    "                activated = self._apply_activation(pooled, 'leaky_relu')\n",
    "            else:\n",
    "                activated = self._apply_activation(pooled, 'relu')\n",
    "            \n",
    "            branch_outputs.append(activated)\n",
    "        \n",
    "        # Concatenate all branch outputs\n",
    "        concatenated_features = torch.cat(branch_outputs, dim=1)\n",
    "        \n",
    "        # Apply correlation learning\n",
    "        correlated_features = self.correlation_net(concatenated_features)\n",
    "        \n",
    "        # Apply fast correlation feature selection\n",
    "        selected_features = self.feature_selector(correlated_features)\n",
    "        \n",
    "        # Global average pooling\n",
    "        global_features = F.adaptive_avg_pool2d(selected_features, (1, 1))\n",
    "        global_features = global_features.view(batch_size, -1)\n",
    "        \n",
    "        # Classification\n",
    "        classification_logits = self.classifier(global_features)\n",
    "        \n",
    "        return global_features, classification_logits\n",
    "\n",
    "class CorrelationNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Neural network component that evaluates and correlates CNN outputs\n",
    "    to improve feature relevance and classification confidence\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_channels: int, output_channels: int):\n",
    "        super(CorrelationNetwork, self).__init__()\n",
    "        \n",
    "        # FIXED: Use the actual input_channels instead of hardcoded 512\n",
    "        self.correlation_layers = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, 384, 3, padding=1),\n",
    "            nn.BatchNorm2d(384),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Conv2d(384, 256, 3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Conv2d(256, output_channels, 1),\n",
    "            nn.BatchNorm2d(output_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        \n",
    "        # Attention mechanism for feature correlation\n",
    "        self.attention = CorrelationAttention(output_channels)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        features = self.correlation_layers(x)\n",
    "        correlated_features = self.attention(features)\n",
    "        return correlated_features\n",
    "\n",
    "class CorrelationAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Attention mechanism that learns correlations between feature maps\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, channels: int, reduction: int = 16):\n",
    "        super(CorrelationAttention, self).__init__()\n",
    "        \n",
    "        self.channel_attention = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv2d(channels, channels // reduction, 1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(channels // reduction, channels, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        self.spatial_attention = nn.Sequential(\n",
    "            nn.Conv2d(2, 1, 7, padding=3),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Channel attention\n",
    "        channel_weights = self.channel_attention(x)\n",
    "        x_channel = x * channel_weights\n",
    "        \n",
    "        # Spatial attention\n",
    "        avg_out = torch.mean(x_channel, dim=1, keepdim=True)\n",
    "        max_out, _ = torch.max(x_channel, dim=1, keepdim=True)\n",
    "        spatial_input = torch.cat([avg_out, max_out], dim=1)\n",
    "        spatial_weights = self.spatial_attention(spatial_input)\n",
    "        x_spatial = x_channel * spatial_weights\n",
    "        \n",
    "        return x_spatial\n",
    "\n",
    "class FastCorrelationFeatureSelector(nn.Module):\n",
    "    \"\"\"\n",
    "    Fast-correlation filter-based automatic feature selection\n",
    "    Avoids redundancy in features as mentioned in the research\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_channels: int, output_channels: int):\n",
    "        super(FastCorrelationFeatureSelector, self).__init__()\n",
    "        \n",
    "        self.selector = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, input_channels // 2, 1),\n",
    "            nn.BatchNorm2d(input_channels // 2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Conv2d(input_channels // 2, output_channels, 1),\n",
    "            nn.BatchNorm2d(output_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        \n",
    "        # Learnable feature importance weights\n",
    "        self.feature_importance = nn.Parameter(torch.ones(input_channels))\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Apply feature importance weights\n",
    "        weighted_x = x * self.feature_importance.view(1, -1, 1, 1)\n",
    "        \n",
    "        # Feature selection\n",
    "        selected_features = self.selector(weighted_x)\n",
    "        return selected_features\n",
    "\n",
    "class BrainTumorDataset(Dataset):\n",
    "    \"\"\"Dataset class for preprocessed brain tumor CT scans\"\"\"\n",
    "    \n",
    "    def __init__(self, data_dir: str, split: str = \"train\", transform=None):\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.split = split\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Collect all processed files\n",
    "        self.samples = []\n",
    "        for label in [\"tumor\", \"normal\"]:\n",
    "            label_dir = self.data_dir / split / label\n",
    "            if label_dir.exists():\n",
    "                for file_path in label_dir.glob(\"*.npy\"):\n",
    "                    self.samples.append((file_path, 1 if label == \"tumor\" else 0))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        file_path, label = self.samples[idx]\n",
    "        \n",
    "        # Load preprocessed multi-scale data\n",
    "        data = np.load(file_path)  # Shape: (H, W, 4) - multi-channel\n",
    "        data = data.transpose(2, 0, 1)  # Convert to (4, H, W)\n",
    "        \n",
    "        # Convert to tensor\n",
    "        data = torch.FloatTensor(data)\n",
    "        label = torch.LongTensor([label]).squeeze()\n",
    "        \n",
    "        if self.transform:\n",
    "            data = self.transform(data)\n",
    "        \n",
    "        return data, label\n",
    "\n",
    "class FeatureExtractionTrainer:\n",
    "    \"\"\"\n",
    "    Trainer class for the feature extraction and correlation learning step\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model: nn.Module, device: torch.device):\n",
    "        # Move model to device FIRST before initializing optimizer\n",
    "        self.model = model.to(device)\n",
    "        self.device = device\n",
    "        \n",
    "        # Loss function with class weighting for imbalance\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, patience=5, factor=0.5)\n",
    "        \n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.train_accuracies = []\n",
    "        self.val_accuracies = []\n",
    "    \n",
    "    def train_epoch(self, train_loader: DataLoader) -> Tuple[float, float]:\n",
    "        self.model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for batch_idx, (data, targets) in enumerate(train_loader):\n",
    "            data, targets = data.to(self.device), targets.to(self.device)\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            features, outputs = self.model(data)\n",
    "            loss = self.criterion(outputs, targets)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "            \n",
    "            if batch_idx % 50 == 0:\n",
    "                print(f'Batch {batch_idx}, Loss: {loss.item():.4f}')\n",
    "        \n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        epoch_accuracy = 100. * correct / total\n",
    "        \n",
    "        return epoch_loss, epoch_accuracy\n",
    "    \n",
    "    def validate_epoch(self, val_loader: DataLoader) -> Tuple[float, float]:\n",
    "        self.model.eval()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for data, targets in val_loader:\n",
    "                data, targets = data.to(self.device), targets.to(self.device)\n",
    "                \n",
    "                features, outputs = self.model(data)\n",
    "                loss = self.criterion(outputs, targets)\n",
    "                \n",
    "                running_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                total += targets.size(0)\n",
    "                correct += predicted.eq(targets).sum().item()\n",
    "        \n",
    "        epoch_loss = running_loss / len(val_loader)\n",
    "        epoch_accuracy = 100. * correct / total\n",
    "        \n",
    "        return epoch_loss, epoch_accuracy\n",
    "    \n",
    "    def train(self, train_loader: DataLoader, val_loader: DataLoader, epochs: int = 50):\n",
    "        print(\"Starting Feature Extraction and Correlation Learning Training...\")\n",
    "        \n",
    "        best_val_accuracy = 0.0\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Training\n",
    "            train_loss, train_acc = self.train_epoch(train_loader)\n",
    "            self.train_losses.append(train_loss)\n",
    "            self.train_accuracies.append(train_acc)\n",
    "            \n",
    "            # Validation\n",
    "            val_loss, val_acc = self.validate_epoch(val_loader)\n",
    "            self.val_losses.append(val_loss)\n",
    "            self.val_accuracies.append(val_acc)\n",
    "            \n",
    "            # Learning rate scheduling\n",
    "            self.scheduler.step(val_loss)\n",
    "            \n",
    "            print(f'Epoch {epoch+1}/{epochs}:')\n",
    "            print(f'  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%')\n",
    "            print(f'  Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')\n",
    "            print(f'  Learning Rate: {self.optimizer.param_groups[0][\"lr\"]:.6f}')\n",
    "            \n",
    "            # Save best model\n",
    "            if val_acc > best_val_accuracy:\n",
    "                best_val_accuracy = val_acc\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': self.model.state_dict(),\n",
    "                    'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "                    'val_accuracy': val_acc,\n",
    "                }, 'best_clm_model.pth')\n",
    "                print(f'  New best model saved with validation accuracy: {val_acc:.2f}%')\n",
    "            \n",
    "            print('-' * 60)\n",
    "    \n",
    "    def plot_training_history(self):\n",
    "        \"\"\"Plot training and validation metrics\"\"\"\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "        \n",
    "        # Loss plot\n",
    "        ax1.plot(self.train_losses, label='Training Loss')\n",
    "        ax1.plot(self.val_losses, label='Validation Loss')\n",
    "        ax1.set_title('Training and Validation Loss')\n",
    "        ax1.set_xlabel('Epoch')\n",
    "        ax1.set_ylabel('Loss')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True)\n",
    "        \n",
    "        # Accuracy plot\n",
    "        ax2.plot(self.train_accuracies, label='Training Accuracy')\n",
    "        ax2.plot(self.val_accuracies, label='Validation Accuracy')\n",
    "        ax2.set_title('Training and Validation Accuracy')\n",
    "        ax2.set_xlabel('Epoch')\n",
    "        ax2.set_ylabel('Accuracy (%)')\n",
    "        ax2.legend()\n",
    "        ax2.grid(True)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('clm_training_history.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "\n",
    "def analyze_feature_correlations(model: CorrelationLearningMechanism, dataloader: DataLoader, device: torch.device):\n",
    "    \"\"\"\n",
    "    Analyze feature correlations learned by the CLM\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_features = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, targets in dataloader:\n",
    "            data, targets = data.to(device), targets.to(device)\n",
    "            features, _ = model(data)\n",
    "            all_features.append(features.cpu().numpy())\n",
    "            all_labels.append(targets.cpu().numpy())\n",
    "    \n",
    "    all_features = np.vstack(all_features)\n",
    "    all_labels = np.hstack(all_labels)\n",
    "    \n",
    "    print(f\"Extracted features shape: {all_features.shape}\")\n",
    "    \n",
    "    # Calculate feature correlations\n",
    "    correlation_matrix = np.corrcoef(all_features.T)\n",
    "    \n",
    "    # Plot correlation matrix\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    plt.imshow(correlation_matrix, cmap='coolwarm', aspect='auto')\n",
    "    plt.colorbar()\n",
    "    plt.title('Feature Correlation Matrix')\n",
    "    plt.xlabel('Feature Index')\n",
    "    plt.ylabel('Feature Index')\n",
    "    plt.savefig('feature_correlations.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    return all_features, all_labels\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run feature extraction and correlation learning\"\"\"\n",
    "    # Configuration\n",
    "    DATA_DIR = \"/content/Class_project_Early_detection_of_brain_tumor/CT_enhanced\"\n",
    "    BATCH_SIZE = 16\n",
    "    EPOCHS = 50\n",
    "    \n",
    "    # Set device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = BrainTumorDataset(DATA_DIR, split=\"train\")\n",
    "    val_dataset = BrainTumorDataset(DATA_DIR, split=\"val\")\n",
    "    test_dataset = BrainTumorDataset(DATA_DIR, split=\"test\")\n",
    "    \n",
    "    print(f\"Training samples: {len(train_dataset)}\")\n",
    "    print(f\"Validation samples: {len(val_dataset)}\")\n",
    "    print(f\"Test samples: {len(test_dataset)}\")\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "    \n",
    "    # Initialize CLM model and move to device immediately\n",
    "    model = CorrelationLearningMechanism(input_channels=4, num_classes=2)\n",
    "    model = model.to(device)  # Move model to device before any operations\n",
    "    print(f\"Model initialized with {sum(p.numel() for p in model.parameters()):,} parameters\")\n",
    "    \n",
    "    # Test a forward pass with a single batch to verify the fix\n",
    "    print(\"Testing forward pass with a single batch...\")\n",
    "    with torch.no_grad():\n",
    "        test_batch, test_targets = next(iter(train_loader))\n",
    "        test_batch = test_batch.to(device)\n",
    "        # Ensure model is on the same device\n",
    "        model = model.to(device)\n",
    "        features, outputs = model(test_batch)\n",
    "        print(f\"Forward pass successful! Features shape: {features.shape}, Outputs shape: {outputs.shape}\")\n",
    "    \n",
    "    # Initialize trainer (model is already on device)\n",
    "    trainer = FeatureExtractionTrainer(model, device)\n",
    "    \n",
    "    # Train the model\n",
    "    trainer.train(train_loader, val_loader, epochs=EPOCHS)\n",
    "    \n",
    "    # Plot training history\n",
    "    trainer.plot_training_history()\n",
    "    \n",
    "    # Analyze feature correlations\n",
    "    print(\"Analyzing feature correlations...\")\n",
    "    features, labels = analyze_feature_correlations(model, val_loader, device)\n",
    "    \n",
    "    # Load best model for final evaluation\n",
    "    checkpoint = torch.load('best_clm_model.pth', map_location=device)  # Specify device when loading\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model = model.to(device)  # Ensure model is on correct device after loading\n",
    "    \n",
    "    # Final evaluation on test set\n",
    "    test_loss, test_accuracy = trainer.validate_epoch(test_loader)\n",
    "    print(f\"\\nFinal Test Results:\")\n",
    "    print(f\"Test Loss: {test_loss:.4f}\")\n",
    "    print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
    "    \n",
    "    print(\"\\n‚úÖ Feature Extraction and Correlation Learning completed successfully!\")\n",
    "    print(\"üìä Model saved as: best_clm_model.pth\")\n",
    "    print(\"üìà Training history saved as: clm_training_history.png\")\n",
    "    print(\"üîç Feature correlations saved as: feature_correlations.png\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc0be41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "# Mount Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Copy files to Google Drive\n",
    "files_to_save = [\n",
    "    'best_clm_model.pth',\n",
    "    'clm_training_history.png',\n",
    "    'feature_correlations.png',\n",
    "    'training_results.zip'\n",
    "]\n",
    "\n",
    "for file in files_to_save:\n",
    "    source = f'Class_project_Early_detection_of_brain_tumor/{file}'\n",
    "    if os.path.exists(source):\n",
    "        shutil.copy(source, f'/content/drive/MyDrive/{file}')\n",
    "        print(f\"‚úÖ Saved {file} to Google Drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2947ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(\"Current working directory:\", os.getcwd())\n",
    "print(\"Files in current directory:\")\n",
    "for file in os.listdir('.'):\n",
    "    print(f\"  - {file}\")\n",
    "\n",
    "print(\"\\nFiles in Class_project_Early_detection_of_brain_tumor:\")\n",
    "project_path = 'Class_project_Early_detection_of_brain_tumor'\n",
    "if os.path.exists(project_path):\n",
    "    for file in os.listdir(project_path):\n",
    "        print(f\"  - {file}\")\n",
    "else:\n",
    "    print(\"Project directory not found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "00c8079a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /content\n",
      "\n",
      "Files in current directory:\n",
      "  - .config\n",
      "  - Class_project_Early_detection_of_brain_tumor\n",
      "  - sample_data\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "print(\"Current working directory:\", os.getcwd())\n",
    "print(\"\\nFiles in current directory:\")\n",
    "for file in os.listdir('.'):\n",
    "    print(f\"  - {file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
