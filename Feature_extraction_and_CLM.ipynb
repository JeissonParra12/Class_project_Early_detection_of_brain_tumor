{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c098e20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'Class_project_Early_detection_of_brain_tumor'...\n",
      "remote: Enumerating objects: 13902, done.\u001b[K\n",
      "remote: Counting objects: 100% (5/5), done.\u001b[K\n",
      "remote: Compressing objects: 100% (4/4), done.\u001b[K\n",
      "remote: Total 13902 (delta 2), reused 1 (delta 1), pack-reused 13897 (from 3)\u001b[K\n",
      "Receiving objects: 100% (13902/13902), 773.99 MiB | 32.04 MiB/s, done.\n",
      "Resolving deltas: 100% (3855/3855), done.\n",
      "Updating files: 100% (16380/16380), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/JeissonParra12/Class_project_Early_detection_of_brain_tumor.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7622babe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Training samples: 3861\n",
      "Validation samples: 1028\n",
      "Test samples: 987\n",
      "Total output channels from conv filters: 160\n",
      "Model initialized with 1,582,365 parameters\n",
      "Testing forward pass with a single batch...\n",
      "Forward pass successful! Features shape: torch.Size([16, 128]), Outputs shape: torch.Size([16, 2])\n",
      "Starting Feature Extraction and Correlation Learning Training...\n",
      "Batch 0, Loss: 0.7108\n",
      "Batch 50, Loss: 0.6210\n",
      "Batch 100, Loss: 0.4913\n",
      "Batch 150, Loss: 0.3934\n",
      "Batch 200, Loss: 0.3591\n",
      "Epoch 1/50:\n",
      "  Train Loss: 0.5113, Train Acc: 79.02%\n",
      "  Val Loss: 0.8329, Val Acc: 65.66%\n",
      "  Learning Rate: 0.000100\n",
      "  New best model saved with validation accuracy: 65.66%\n",
      "------------------------------------------------------------\n",
      "Batch 0, Loss: 0.4790\n",
      "Batch 50, Loss: 0.4332\n",
      "Batch 100, Loss: 0.3318\n",
      "Batch 150, Loss: 0.2925\n",
      "Batch 200, Loss: 0.5191\n",
      "Epoch 2/50:\n",
      "  Train Loss: 0.3690, Train Acc: 84.43%\n",
      "  Val Loss: 0.3038, Val Acc: 84.82%\n",
      "  Learning Rate: 0.000100\n",
      "  New best model saved with validation accuracy: 84.82%\n",
      "------------------------------------------------------------\n",
      "Batch 0, Loss: 0.5519\n",
      "Batch 50, Loss: 0.5124\n",
      "Batch 100, Loss: 0.1936\n",
      "Batch 150, Loss: 0.3940\n",
      "Batch 200, Loss: 0.4552\n",
      "Epoch 3/50:\n",
      "  Train Loss: 0.3231, Train Acc: 86.79%\n",
      "  Val Loss: 0.3622, Val Acc: 82.68%\n",
      "  Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Batch 0, Loss: 0.2321\n",
      "Batch 50, Loss: 0.6227\n",
      "Batch 100, Loss: 0.3126\n",
      "Batch 150, Loss: 0.3496\n",
      "Batch 200, Loss: 0.3552\n",
      "Epoch 4/50:\n",
      "  Train Loss: 0.2879, Train Acc: 88.16%\n",
      "  Val Loss: 2.8006, Val Acc: 35.60%\n",
      "  Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Batch 0, Loss: 0.5481\n",
      "Batch 50, Loss: 0.2475\n",
      "Batch 100, Loss: 0.1066\n",
      "Batch 150, Loss: 0.0821\n",
      "Batch 200, Loss: 0.2581\n",
      "Epoch 5/50:\n",
      "  Train Loss: 0.2593, Train Acc: 89.20%\n",
      "  Val Loss: 1.6358, Val Acc: 41.54%\n",
      "  Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Batch 0, Loss: 0.0689\n",
      "Batch 50, Loss: 0.0685\n",
      "Batch 100, Loss: 0.2095\n",
      "Batch 150, Loss: 0.1035\n",
      "Batch 200, Loss: 0.6411\n",
      "Epoch 6/50:\n",
      "  Train Loss: 0.2394, Train Acc: 90.60%\n",
      "  Val Loss: 1.0203, Val Acc: 65.86%\n",
      "  Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Batch 0, Loss: 0.1995\n",
      "Batch 50, Loss: 0.2140\n",
      "Batch 100, Loss: 0.1235\n",
      "Batch 150, Loss: 0.2647\n",
      "Batch 200, Loss: 0.2966\n",
      "Epoch 7/50:\n",
      "  Train Loss: 0.2188, Train Acc: 91.14%\n",
      "  Val Loss: 0.8166, Val Acc: 63.23%\n",
      "  Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Batch 0, Loss: 0.1626\n",
      "Batch 50, Loss: 0.3787\n",
      "Batch 100, Loss: 0.2879\n",
      "Batch 150, Loss: 0.1913\n",
      "Batch 200, Loss: 0.4194\n",
      "Epoch 8/50:\n",
      "  Train Loss: 0.2113, Train Acc: 91.69%\n",
      "  Val Loss: 0.2891, Val Acc: 84.73%\n",
      "  Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Batch 0, Loss: 0.1520\n",
      "Batch 50, Loss: 0.3660\n",
      "Batch 100, Loss: 0.2813\n",
      "Batch 150, Loss: 0.3076\n",
      "Batch 200, Loss: 0.1826\n",
      "Epoch 9/50:\n",
      "  Train Loss: 0.1875, Train Acc: 92.62%\n",
      "  Val Loss: 0.1751, Val Acc: 92.51%\n",
      "  Learning Rate: 0.000100\n",
      "  New best model saved with validation accuracy: 92.51%\n",
      "------------------------------------------------------------\n",
      "Batch 0, Loss: 0.3101\n",
      "Batch 50, Loss: 0.0521\n",
      "Batch 100, Loss: 0.4774\n",
      "Batch 150, Loss: 0.0323\n",
      "Batch 200, Loss: 0.0955\n",
      "Epoch 10/50:\n",
      "  Train Loss: 0.1898, Train Acc: 92.51%\n",
      "  Val Loss: 0.2774, Val Acc: 85.51%\n",
      "  Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Batch 0, Loss: 0.2076\n",
      "Batch 50, Loss: 0.1291\n",
      "Batch 100, Loss: 0.3951\n",
      "Batch 150, Loss: 0.2156\n",
      "Batch 200, Loss: 0.0378\n",
      "Epoch 11/50:\n",
      "  Train Loss: 0.1722, Train Acc: 93.24%\n",
      "  Val Loss: 0.1269, Val Acc: 95.33%\n",
      "  Learning Rate: 0.000100\n",
      "  New best model saved with validation accuracy: 95.33%\n",
      "------------------------------------------------------------\n",
      "Batch 0, Loss: 0.0904\n",
      "Batch 50, Loss: 0.0363\n",
      "Batch 100, Loss: 0.0942\n",
      "Batch 150, Loss: 0.0534\n",
      "Batch 200, Loss: 0.3771\n",
      "Epoch 12/50:\n",
      "  Train Loss: 0.1479, Train Acc: 94.43%\n",
      "  Val Loss: 0.8938, Val Acc: 68.77%\n",
      "  Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Batch 0, Loss: 0.3396\n",
      "Batch 50, Loss: 0.1842\n",
      "Batch 100, Loss: 0.2541\n",
      "Batch 150, Loss: 0.2709\n",
      "Batch 200, Loss: 0.1429\n",
      "Epoch 13/50:\n",
      "  Train Loss: 0.1672, Train Acc: 93.40%\n",
      "  Val Loss: 0.1381, Val Acc: 94.55%\n",
      "  Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Batch 0, Loss: 0.0319\n",
      "Batch 50, Loss: 0.0817\n",
      "Batch 100, Loss: 0.0361\n",
      "Batch 150, Loss: 0.1238\n",
      "Batch 200, Loss: 0.0626\n",
      "Epoch 14/50:\n",
      "  Train Loss: 0.1471, Train Acc: 94.20%\n",
      "  Val Loss: 0.2657, Val Acc: 87.94%\n",
      "  Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Batch 0, Loss: 0.1264\n",
      "Batch 50, Loss: 0.2331\n",
      "Batch 100, Loss: 0.2296\n",
      "Batch 150, Loss: 0.0720\n",
      "Batch 200, Loss: 0.0726\n",
      "Epoch 15/50:\n",
      "  Train Loss: 0.1436, Train Acc: 94.54%\n",
      "  Val Loss: 0.1811, Val Acc: 95.14%\n",
      "  Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Batch 0, Loss: 0.0481\n",
      "Batch 50, Loss: 0.1732\n",
      "Batch 100, Loss: 0.0389\n",
      "Batch 150, Loss: 0.3665\n",
      "Batch 200, Loss: 0.0473\n",
      "Epoch 16/50:\n",
      "  Train Loss: 0.1546, Train Acc: 94.64%\n",
      "  Val Loss: 1.9740, Val Acc: 66.63%\n",
      "  Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Batch 0, Loss: 0.4352\n",
      "Batch 50, Loss: 0.1133\n",
      "Batch 100, Loss: 0.2332\n",
      "Batch 150, Loss: 0.0907\n",
      "Batch 200, Loss: 0.0905\n",
      "Epoch 17/50:\n",
      "  Train Loss: 0.1482, Train Acc: 94.20%\n",
      "  Val Loss: 0.3342, Val Acc: 86.28%\n",
      "  Learning Rate: 0.000050\n",
      "------------------------------------------------------------\n",
      "Batch 0, Loss: 0.1102\n",
      "Batch 50, Loss: 0.0469\n",
      "Batch 100, Loss: 0.0271\n",
      "Batch 150, Loss: 0.1739\n",
      "Batch 200, Loss: 0.1355\n",
      "Epoch 18/50:\n",
      "  Train Loss: 0.1174, Train Acc: 95.55%\n",
      "  Val Loss: 0.0890, Val Acc: 96.11%\n",
      "  Learning Rate: 0.000050\n",
      "  New best model saved with validation accuracy: 96.11%\n",
      "------------------------------------------------------------\n",
      "Batch 0, Loss: 0.3210\n",
      "Batch 50, Loss: 0.0694\n",
      "Batch 100, Loss: 0.0943\n",
      "Batch 150, Loss: 0.0248\n",
      "Batch 200, Loss: 0.0474\n",
      "Epoch 19/50:\n",
      "  Train Loss: 0.1025, Train Acc: 96.30%\n",
      "  Val Loss: 0.4539, Val Acc: 82.00%\n",
      "  Learning Rate: 0.000050\n",
      "------------------------------------------------------------\n",
      "Batch 0, Loss: 0.0588\n",
      "Batch 50, Loss: 0.0980\n",
      "Batch 100, Loss: 0.0283\n",
      "Batch 150, Loss: 0.0478\n",
      "Batch 200, Loss: 0.0545\n",
      "Epoch 20/50:\n",
      "  Train Loss: 0.0995, Train Acc: 96.58%\n",
      "  Val Loss: 0.0818, Val Acc: 96.50%\n",
      "  Learning Rate: 0.000050\n",
      "  New best model saved with validation accuracy: 96.50%\n",
      "------------------------------------------------------------\n",
      "Batch 0, Loss: 0.0338\n",
      "Batch 50, Loss: 0.1368\n",
      "Batch 100, Loss: 0.0312\n",
      "Batch 150, Loss: 0.0596\n",
      "Batch 200, Loss: 0.0930\n",
      "Epoch 21/50:\n",
      "  Train Loss: 0.1005, Train Acc: 96.17%\n",
      "  Val Loss: 0.1132, Val Acc: 95.53%\n",
      "  Learning Rate: 0.000050\n",
      "------------------------------------------------------------\n",
      "Batch 0, Loss: 0.1249\n",
      "Batch 50, Loss: 0.0660\n",
      "Batch 100, Loss: 0.0302\n",
      "Batch 150, Loss: 0.1051\n",
      "Batch 200, Loss: 0.1127\n",
      "Epoch 22/50:\n",
      "  Train Loss: 0.0979, Train Acc: 96.11%\n",
      "  Val Loss: 0.1024, Val Acc: 95.62%\n",
      "  Learning Rate: 0.000050\n",
      "------------------------------------------------------------\n",
      "Batch 0, Loss: 0.0161\n",
      "Batch 50, Loss: 0.0759\n",
      "Batch 100, Loss: 0.0223\n",
      "Batch 150, Loss: 0.0615\n",
      "Batch 200, Loss: 0.0916\n",
      "Epoch 23/50:\n",
      "  Train Loss: 0.0974, Train Acc: 96.40%\n",
      "  Val Loss: 0.0618, Val Acc: 97.67%\n",
      "  Learning Rate: 0.000050\n",
      "  New best model saved with validation accuracy: 97.67%\n",
      "------------------------------------------------------------\n",
      "Batch 0, Loss: 0.0105\n",
      "Batch 50, Loss: 0.0591\n",
      "Batch 100, Loss: 0.1975\n",
      "Batch 150, Loss: 0.0160\n",
      "Batch 200, Loss: 0.0954\n",
      "Epoch 24/50:\n",
      "  Train Loss: 0.0860, Train Acc: 96.79%\n",
      "  Val Loss: 0.0773, Val Acc: 97.08%\n",
      "  Learning Rate: 0.000050\n",
      "------------------------------------------------------------\n",
      "Batch 0, Loss: 0.0299\n",
      "Batch 50, Loss: 0.0760\n",
      "Batch 100, Loss: 0.0923\n",
      "Batch 150, Loss: 0.0848\n",
      "Batch 200, Loss: 0.0919\n",
      "Epoch 25/50:\n",
      "  Train Loss: 0.0886, Train Acc: 96.68%\n",
      "  Val Loss: 0.0800, Val Acc: 96.60%\n",
      "  Learning Rate: 0.000050\n",
      "------------------------------------------------------------\n",
      "Batch 0, Loss: 0.0155\n",
      "Batch 50, Loss: 0.1085\n",
      "Batch 100, Loss: 0.0390\n",
      "Batch 150, Loss: 0.1596\n",
      "Batch 200, Loss: 0.0586\n",
      "Epoch 26/50:\n",
      "  Train Loss: 0.0895, Train Acc: 96.50%\n",
      "  Val Loss: 0.1818, Val Acc: 93.00%\n",
      "  Learning Rate: 0.000050\n",
      "------------------------------------------------------------\n",
      "Batch 0, Loss: 0.0973\n",
      "Batch 50, Loss: 0.0470\n",
      "Batch 100, Loss: 0.0314\n",
      "Batch 150, Loss: 0.0623\n",
      "Batch 200, Loss: 0.0344\n",
      "Epoch 27/50:\n",
      "  Train Loss: 0.0736, Train Acc: 97.23%\n",
      "  Val Loss: 0.1213, Val Acc: 94.55%\n",
      "  Learning Rate: 0.000050\n",
      "------------------------------------------------------------\n",
      "Batch 0, Loss: 0.0210\n",
      "Batch 50, Loss: 0.0327\n",
      "Batch 100, Loss: 0.0134\n",
      "Batch 150, Loss: 0.1664\n",
      "Batch 200, Loss: 0.0107\n",
      "Epoch 28/50:\n",
      "  Train Loss: 0.0771, Train Acc: 97.36%\n",
      "  Val Loss: 0.1534, Val Acc: 93.87%\n",
      "  Learning Rate: 0.000050\n",
      "------------------------------------------------------------\n",
      "Batch 0, Loss: 0.0670\n",
      "Batch 50, Loss: 0.0254\n",
      "Batch 100, Loss: 0.2401\n",
      "Batch 150, Loss: 0.3339\n",
      "Batch 200, Loss: 0.0323\n",
      "Epoch 29/50:\n",
      "  Train Loss: 0.0794, Train Acc: 97.15%\n",
      "  Val Loss: 0.0961, Val Acc: 96.11%\n",
      "  Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Batch 0, Loss: 0.0386\n",
      "Batch 50, Loss: 0.0421\n",
      "Batch 100, Loss: 0.0473\n",
      "Batch 150, Loss: 0.0853\n",
      "Batch 200, Loss: 0.1286\n",
      "Epoch 30/50:\n",
      "  Train Loss: 0.0588, Train Acc: 97.90%\n",
      "  Val Loss: 0.0619, Val Acc: 97.76%\n",
      "  Learning Rate: 0.000025\n",
      "  New best model saved with validation accuracy: 97.76%\n",
      "------------------------------------------------------------\n",
      "Batch 0, Loss: 0.0794\n",
      "Batch 50, Loss: 0.0250\n",
      "Batch 100, Loss: 0.0072\n",
      "Batch 150, Loss: 0.0113\n",
      "Batch 200, Loss: 0.0275\n",
      "Epoch 31/50:\n",
      "  Train Loss: 0.0542, Train Acc: 98.06%\n",
      "  Val Loss: 0.0511, Val Acc: 98.25%\n",
      "  Learning Rate: 0.000025\n",
      "  New best model saved with validation accuracy: 98.25%\n",
      "------------------------------------------------------------\n",
      "Batch 0, Loss: 0.0529\n",
      "Batch 50, Loss: 0.0057\n",
      "Batch 100, Loss: 0.0123\n",
      "Batch 150, Loss: 0.0444\n",
      "Batch 200, Loss: 0.0045\n",
      "Epoch 32/50:\n",
      "  Train Loss: 0.0607, Train Acc: 97.88%\n",
      "  Val Loss: 0.0636, Val Acc: 97.67%\n",
      "  Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Batch 0, Loss: 0.0638\n",
      "Batch 50, Loss: 0.0607\n",
      "Batch 100, Loss: 0.0852\n",
      "Batch 150, Loss: 0.1276\n",
      "Batch 200, Loss: 0.2043\n",
      "Epoch 33/50:\n",
      "  Train Loss: 0.0511, Train Acc: 98.52%\n",
      "  Val Loss: 0.0429, Val Acc: 98.35%\n",
      "  Learning Rate: 0.000025\n",
      "  New best model saved with validation accuracy: 98.35%\n",
      "------------------------------------------------------------\n",
      "Batch 0, Loss: 0.0104\n",
      "Batch 50, Loss: 0.0580\n",
      "Batch 100, Loss: 0.0123\n",
      "Batch 150, Loss: 0.0624\n",
      "Batch 200, Loss: 0.0038\n",
      "Epoch 34/50:\n",
      "  Train Loss: 0.0500, Train Acc: 98.34%\n",
      "  Val Loss: 0.0555, Val Acc: 98.05%\n",
      "  Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Batch 0, Loss: 0.0034\n",
      "Batch 50, Loss: 0.0104\n",
      "Batch 100, Loss: 0.0143\n",
      "Batch 150, Loss: 0.0938\n",
      "Batch 200, Loss: 0.0022\n",
      "Epoch 35/50:\n",
      "  Train Loss: 0.0562, Train Acc: 98.32%\n",
      "  Val Loss: 0.1033, Val Acc: 96.60%\n",
      "  Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Batch 0, Loss: 0.0644\n",
      "Batch 50, Loss: 0.0559\n",
      "Batch 100, Loss: 0.0187\n",
      "Batch 150, Loss: 0.0068\n",
      "Batch 200, Loss: 0.0053\n",
      "Epoch 36/50:\n",
      "  Train Loss: 0.0571, Train Acc: 98.01%\n",
      "  Val Loss: 0.0564, Val Acc: 98.35%\n",
      "  Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Batch 0, Loss: 0.0519\n",
      "Batch 50, Loss: 0.0420\n",
      "Batch 100, Loss: 0.0420\n",
      "Batch 150, Loss: 0.0255\n",
      "Batch 200, Loss: 0.0085\n",
      "Epoch 37/50:\n",
      "  Train Loss: 0.0445, Train Acc: 98.63%\n",
      "  Val Loss: 0.0498, Val Acc: 98.35%\n",
      "  Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Batch 0, Loss: 0.0232\n",
      "Batch 50, Loss: 0.0157\n",
      "Batch 100, Loss: 0.0358\n",
      "Batch 150, Loss: 0.0029\n",
      "Batch 200, Loss: 0.0376\n",
      "Epoch 38/50:\n",
      "  Train Loss: 0.0400, Train Acc: 98.68%\n",
      "  Val Loss: 0.0583, Val Acc: 97.76%\n",
      "  Learning Rate: 0.000025\n",
      "------------------------------------------------------------\n",
      "Batch 0, Loss: 0.0140\n",
      "Batch 50, Loss: 0.0944\n",
      "Batch 100, Loss: 0.0266\n",
      "Batch 150, Loss: 0.0133\n",
      "Batch 200, Loss: 0.1085\n",
      "Epoch 39/50:\n",
      "  Train Loss: 0.0450, Train Acc: 98.70%\n",
      "  Val Loss: 0.0664, Val Acc: 97.47%\n",
      "  Learning Rate: 0.000013\n",
      "------------------------------------------------------------\n",
      "Batch 0, Loss: 0.0611\n",
      "Batch 50, Loss: 0.0082\n",
      "Batch 100, Loss: 0.0206\n",
      "Batch 150, Loss: 0.0954\n",
      "Batch 200, Loss: 0.3966\n",
      "Epoch 40/50:\n",
      "  Train Loss: 0.0359, Train Acc: 98.89%\n",
      "  Val Loss: 0.0381, Val Acc: 98.64%\n",
      "  Learning Rate: 0.000013\n",
      "  New best model saved with validation accuracy: 98.64%\n",
      "------------------------------------------------------------\n",
      "Batch 0, Loss: 0.0402\n",
      "Batch 50, Loss: 0.0146\n",
      "Batch 100, Loss: 0.1044\n",
      "Batch 150, Loss: 0.0257\n",
      "Batch 200, Loss: 0.0073\n",
      "Epoch 41/50:\n",
      "  Train Loss: 0.0325, Train Acc: 99.15%\n",
      "  Val Loss: 0.0796, Val Acc: 97.18%\n",
      "  Learning Rate: 0.000013\n",
      "------------------------------------------------------------\n",
      "Batch 0, Loss: 0.0279\n",
      "Batch 50, Loss: 0.0184\n",
      "Batch 100, Loss: 0.0208\n",
      "Batch 150, Loss: 0.0255\n",
      "Batch 200, Loss: 0.0899\n",
      "Epoch 42/50:\n",
      "  Train Loss: 0.0324, Train Acc: 98.99%\n",
      "  Val Loss: 0.0614, Val Acc: 97.28%\n",
      "  Learning Rate: 0.000013\n",
      "------------------------------------------------------------\n",
      "Batch 0, Loss: 0.0123\n",
      "Batch 50, Loss: 0.0085\n",
      "Batch 100, Loss: 0.0044\n",
      "Batch 150, Loss: 0.1310\n",
      "Batch 200, Loss: 0.0056\n",
      "Epoch 43/50:\n",
      "  Train Loss: 0.0352, Train Acc: 99.12%\n",
      "  Val Loss: 0.0402, Val Acc: 98.74%\n",
      "  Learning Rate: 0.000013\n",
      "  New best model saved with validation accuracy: 98.74%\n",
      "------------------------------------------------------------\n",
      "Batch 0, Loss: 0.0040\n",
      "Batch 50, Loss: 0.0030\n",
      "Batch 100, Loss: 0.0072\n",
      "Batch 150, Loss: 0.0063\n",
      "Batch 200, Loss: 0.0080\n",
      "Epoch 44/50:\n",
      "  Train Loss: 0.0297, Train Acc: 99.25%\n",
      "  Val Loss: 0.0434, Val Acc: 98.64%\n",
      "  Learning Rate: 0.000013\n",
      "------------------------------------------------------------\n",
      "Batch 0, Loss: 0.0171\n",
      "Batch 50, Loss: 0.0088\n",
      "Batch 100, Loss: 0.0502\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from scipy.stats import pearsonr\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class CorrelationLearningMechanism(nn.Module):\n",
    "    \"\"\"\n",
    "    Correlation Learning Mechanism (CLM) inspired by Wo≈∫niak et al. (2023)\n",
    "    Dynamically filters convolutional layer combinations and evaluates feature correlations\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_channels: int = 4, num_classes: int = 2):\n",
    "        super(CorrelationLearningMechanism, self).__init__()\n",
    "        \n",
    "        self.input_channels = input_channels\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # Multi-scale feature extraction branches\n",
    "        self.branch_configs = self._create_branch_configurations()\n",
    "        \n",
    "        # Dynamic convolutional filter banks\n",
    "        self.conv_filters = nn.ModuleDict()\n",
    "        self._initialize_conv_filters()\n",
    "        \n",
    "        # Calculate the actual output channels from conv_filters\n",
    "        total_output_channels = 0\n",
    "        for name in self.conv_filters:\n",
    "            if 'standard_3x3' in name:\n",
    "                total_output_channels += 32\n",
    "            elif 'standard_5x5' in name:\n",
    "                total_output_channels += 32\n",
    "            elif 'depthwise_3x3' in name:\n",
    "                total_output_channels += 32\n",
    "            elif 'dilated_3x3' in name:\n",
    "                total_output_channels += 32\n",
    "            elif 'asymmetric_1x3' in name:\n",
    "                total_output_channels += 32\n",
    "        \n",
    "        print(f\"Total output channels from conv filters: {total_output_channels}\")\n",
    "        \n",
    "        # Correlation learning components - FIXED CHANNEL DIMENSIONS\n",
    "        self.correlation_net = CorrelationNetwork(total_output_channels, 256)\n",
    "        self.feature_selector = FastCorrelationFeatureSelector(256, 128)\n",
    "        \n",
    "        # Classification head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(64, num_classes)\n",
    "        )\n",
    "        \n",
    "    def _create_branch_configurations(self) -> List[Dict]:\n",
    "        \"\"\"Create different branch configurations for dynamic filtering\"\"\"\n",
    "        configs = [\n",
    "            # Branch 1: Standard convolution\n",
    "            {'filters': 32, 'kernel_size': 3, 'pool_type': 'max', 'activation': 'relu'},\n",
    "            # Branch 2: Depth-wise separable convolution\n",
    "            {'filters': 64, 'kernel_size': 5, 'pool_type': 'avg', 'activation': 'relu'},\n",
    "            # Branch 3: Dilated convolution for larger receptive field\n",
    "            {'filters': 32, 'kernel_size': 3, 'dilation': 2, 'pool_type': 'max', 'activation': 'leaky_relu'},\n",
    "            # Branch 4: Asymmetric convolution\n",
    "            {'filters': 64, 'kernel_size': (1, 3), 'pool_type': 'avg', 'activation': 'relu'},\n",
    "        ]\n",
    "        return configs\n",
    "    \n",
    "    def _initialize_conv_filters(self):\n",
    "        \"\"\"Initialize different convolutional filter types\"\"\"\n",
    "        # Standard convolutional layers\n",
    "        self.conv_filters['standard_3x3'] = nn.Conv2d(self.input_channels, 32, 3, padding=1)\n",
    "        self.conv_filters['standard_5x5'] = nn.Conv2d(self.input_channels, 32, 5, padding=2)\n",
    "        \n",
    "        # Depth-wise separable convolutions\n",
    "        self.conv_filters['depthwise_3x3'] = nn.Sequential(\n",
    "            nn.Conv2d(self.input_channels, self.input_channels, 3, padding=1, groups=self.input_channels),\n",
    "            nn.Conv2d(self.input_channels, 32, 1)\n",
    "        )\n",
    "        \n",
    "        # Dilated convolutions\n",
    "        self.conv_filters['dilated_3x3'] = nn.Conv2d(self.input_channels, 32, 3, padding=2, dilation=2)\n",
    "        \n",
    "        # Asymmetric convolutions\n",
    "        self.conv_filters['asymmetric_1x3'] = nn.Sequential(\n",
    "            nn.Conv2d(self.input_channels, 32, (1, 3), padding=(0, 1)),\n",
    "            nn.Conv2d(32, 32, (3, 1), padding=(1, 0))\n",
    "        )\n",
    "    \n",
    "    def _apply_dynamic_pooling(self, x: torch.Tensor, pool_type: str) -> torch.Tensor:\n",
    "        \"\"\"Apply dynamic pooling operations\"\"\"\n",
    "        if pool_type == 'max':\n",
    "            return F.adaptive_max_pool2d(x, (x.size(2)//2, x.size(3)//2))\n",
    "        elif pool_type == 'avg':\n",
    "            return F.adaptive_avg_pool2d(x, (x.size(2)//2, x.size(3)//2))\n",
    "        elif pool_type == 'mixed':\n",
    "            max_pool = F.adaptive_max_pool2d(x, (x.size(2)//2, x.size(3)//2))\n",
    "            avg_pool = F.adaptive_avg_pool2d(x, (x.size(2)//2, x.size(3)//2))\n",
    "            return (max_pool + avg_pool) / 2\n",
    "        else:\n",
    "            return x\n",
    "    \n",
    "    def _apply_activation(self, x: torch.Tensor, activation: str) -> torch.Tensor:\n",
    "        \"\"\"Apply dynamic activation functions\"\"\"\n",
    "        if activation == 'relu':\n",
    "            return F.relu(x)\n",
    "        elif activation == 'leaky_relu':\n",
    "            return F.leaky_relu(x, 0.1)\n",
    "        elif activation == 'elu':\n",
    "            return F.elu(x)\n",
    "        elif activation == 'selu':\n",
    "            return F.selu(x)\n",
    "        else:\n",
    "            return x\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Forward pass through CLM\n",
    "        Returns: tuple of (features, classification_logits)\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # Extract features using multiple filter configurations\n",
    "        branch_outputs = []\n",
    "        \n",
    "        for name, filter_module in self.conv_filters.items():\n",
    "            # Apply convolutional filter\n",
    "            filtered = filter_module(x)\n",
    "            \n",
    "            # Apply pooling based on filter type\n",
    "            if 'dilated' in name:\n",
    "                pooled = self._apply_dynamic_pooling(filtered, 'max')\n",
    "            elif 'depthwise' in name:\n",
    "                pooled = self._apply_dynamic_pooling(filtered, 'avg')\n",
    "            else:\n",
    "                pooled = self._apply_dynamic_pooling(filtered, 'mixed')\n",
    "            \n",
    "            # Apply activation\n",
    "            if 'leaky' in name:\n",
    "                activated = self._apply_activation(pooled, 'leaky_relu')\n",
    "            else:\n",
    "                activated = self._apply_activation(pooled, 'relu')\n",
    "            \n",
    "            branch_outputs.append(activated)\n",
    "        \n",
    "        # Concatenate all branch outputs\n",
    "        concatenated_features = torch.cat(branch_outputs, dim=1)\n",
    "        \n",
    "        # Apply correlation learning\n",
    "        correlated_features = self.correlation_net(concatenated_features)\n",
    "        \n",
    "        # Apply fast correlation feature selection\n",
    "        selected_features = self.feature_selector(correlated_features)\n",
    "        \n",
    "        # Global average pooling\n",
    "        global_features = F.adaptive_avg_pool2d(selected_features, (1, 1))\n",
    "        global_features = global_features.view(batch_size, -1)\n",
    "        \n",
    "        # Classification\n",
    "        classification_logits = self.classifier(global_features)\n",
    "        \n",
    "        return global_features, classification_logits\n",
    "\n",
    "class CorrelationNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Neural network component that evaluates and correlates CNN outputs\n",
    "    to improve feature relevance and classification confidence\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_channels: int, output_channels: int):\n",
    "        super(CorrelationNetwork, self).__init__()\n",
    "        \n",
    "        # FIXED: Use the actual input_channels instead of hardcoded 512\n",
    "        self.correlation_layers = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, 384, 3, padding=1),\n",
    "            nn.BatchNorm2d(384),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Conv2d(384, 256, 3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Conv2d(256, output_channels, 1),\n",
    "            nn.BatchNorm2d(output_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        \n",
    "        # Attention mechanism for feature correlation\n",
    "        self.attention = CorrelationAttention(output_channels)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        features = self.correlation_layers(x)\n",
    "        correlated_features = self.attention(features)\n",
    "        return correlated_features\n",
    "\n",
    "class CorrelationAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Attention mechanism that learns correlations between feature maps\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, channels: int, reduction: int = 16):\n",
    "        super(CorrelationAttention, self).__init__()\n",
    "        \n",
    "        self.channel_attention = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv2d(channels, channels // reduction, 1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(channels // reduction, channels, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        self.spatial_attention = nn.Sequential(\n",
    "            nn.Conv2d(2, 1, 7, padding=3),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Channel attention\n",
    "        channel_weights = self.channel_attention(x)\n",
    "        x_channel = x * channel_weights\n",
    "        \n",
    "        # Spatial attention\n",
    "        avg_out = torch.mean(x_channel, dim=1, keepdim=True)\n",
    "        max_out, _ = torch.max(x_channel, dim=1, keepdim=True)\n",
    "        spatial_input = torch.cat([avg_out, max_out], dim=1)\n",
    "        spatial_weights = self.spatial_attention(spatial_input)\n",
    "        x_spatial = x_channel * spatial_weights\n",
    "        \n",
    "        return x_spatial\n",
    "\n",
    "class FastCorrelationFeatureSelector(nn.Module):\n",
    "    \"\"\"\n",
    "    Fast-correlation filter-based automatic feature selection\n",
    "    Avoids redundancy in features as mentioned in the research\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_channels: int, output_channels: int):\n",
    "        super(FastCorrelationFeatureSelector, self).__init__()\n",
    "        \n",
    "        self.selector = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, input_channels // 2, 1),\n",
    "            nn.BatchNorm2d(input_channels // 2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Conv2d(input_channels // 2, output_channels, 1),\n",
    "            nn.BatchNorm2d(output_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        \n",
    "        # Learnable feature importance weights\n",
    "        self.feature_importance = nn.Parameter(torch.ones(input_channels))\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Apply feature importance weights\n",
    "        weighted_x = x * self.feature_importance.view(1, -1, 1, 1)\n",
    "        \n",
    "        # Feature selection\n",
    "        selected_features = self.selector(weighted_x)\n",
    "        return selected_features\n",
    "\n",
    "class BrainTumorDataset(Dataset):\n",
    "    \"\"\"Dataset class for preprocessed brain tumor CT scans\"\"\"\n",
    "    \n",
    "    def __init__(self, data_dir: str, split: str = \"train\", transform=None):\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.split = split\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Collect all processed files\n",
    "        self.samples = []\n",
    "        for label in [\"tumor\", \"normal\"]:\n",
    "            label_dir = self.data_dir / split / label\n",
    "            if label_dir.exists():\n",
    "                for file_path in label_dir.glob(\"*.npy\"):\n",
    "                    self.samples.append((file_path, 1 if label == \"tumor\" else 0))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        file_path, label = self.samples[idx]\n",
    "        \n",
    "        # Load preprocessed multi-scale data\n",
    "        data = np.load(file_path)  # Shape: (H, W, 4) - multi-channel\n",
    "        data = data.transpose(2, 0, 1)  # Convert to (4, H, W)\n",
    "        \n",
    "        # Convert to tensor\n",
    "        data = torch.FloatTensor(data)\n",
    "        label = torch.LongTensor([label]).squeeze()\n",
    "        \n",
    "        if self.transform:\n",
    "            data = self.transform(data)\n",
    "        \n",
    "        return data, label\n",
    "\n",
    "class FeatureExtractionTrainer:\n",
    "    \"\"\"\n",
    "    Trainer class for the feature extraction and correlation learning step\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model: nn.Module, device: torch.device):\n",
    "        # Move model to device FIRST before initializing optimizer\n",
    "        self.model = model.to(device)\n",
    "        self.device = device\n",
    "        \n",
    "        # Loss function with class weighting for imbalance\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, patience=5, factor=0.5)\n",
    "        \n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.train_accuracies = []\n",
    "        self.val_accuracies = []\n",
    "    \n",
    "    def train_epoch(self, train_loader: DataLoader) -> Tuple[float, float]:\n",
    "        self.model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for batch_idx, (data, targets) in enumerate(train_loader):\n",
    "            data, targets = data.to(self.device), targets.to(self.device)\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            features, outputs = self.model(data)\n",
    "            loss = self.criterion(outputs, targets)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "            \n",
    "            if batch_idx % 50 == 0:\n",
    "                print(f'Batch {batch_idx}, Loss: {loss.item():.4f}')\n",
    "        \n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        epoch_accuracy = 100. * correct / total\n",
    "        \n",
    "        return epoch_loss, epoch_accuracy\n",
    "    \n",
    "    def validate_epoch(self, val_loader: DataLoader) -> Tuple[float, float]:\n",
    "        self.model.eval()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for data, targets in val_loader:\n",
    "                data, targets = data.to(self.device), targets.to(self.device)\n",
    "                \n",
    "                features, outputs = self.model(data)\n",
    "                loss = self.criterion(outputs, targets)\n",
    "                \n",
    "                running_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                total += targets.size(0)\n",
    "                correct += predicted.eq(targets).sum().item()\n",
    "        \n",
    "        epoch_loss = running_loss / len(val_loader)\n",
    "        epoch_accuracy = 100. * correct / total\n",
    "        \n",
    "        return epoch_loss, epoch_accuracy\n",
    "    \n",
    "    def train(self, train_loader: DataLoader, val_loader: DataLoader, epochs: int = 50):\n",
    "        print(\"Starting Feature Extraction and Correlation Learning Training...\")\n",
    "        \n",
    "        best_val_accuracy = 0.0\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Training\n",
    "            train_loss, train_acc = self.train_epoch(train_loader)\n",
    "            self.train_losses.append(train_loss)\n",
    "            self.train_accuracies.append(train_acc)\n",
    "            \n",
    "            # Validation\n",
    "            val_loss, val_acc = self.validate_epoch(val_loader)\n",
    "            self.val_losses.append(val_loss)\n",
    "            self.val_accuracies.append(val_acc)\n",
    "            \n",
    "            # Learning rate scheduling\n",
    "            self.scheduler.step(val_loss)\n",
    "            \n",
    "            print(f'Epoch {epoch+1}/{epochs}:')\n",
    "            print(f'  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%')\n",
    "            print(f'  Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')\n",
    "            print(f'  Learning Rate: {self.optimizer.param_groups[0][\"lr\"]:.6f}')\n",
    "            \n",
    "            # Save best model\n",
    "            if val_acc > best_val_accuracy:\n",
    "                best_val_accuracy = val_acc\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': self.model.state_dict(),\n",
    "                    'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "                    'val_accuracy': val_acc,\n",
    "                }, 'best_clm_model.pth')\n",
    "                print(f'  New best model saved with validation accuracy: {val_acc:.2f}%')\n",
    "            \n",
    "            print('-' * 60)\n",
    "    \n",
    "    def plot_training_history(self):\n",
    "        \"\"\"Plot training and validation metrics\"\"\"\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "        \n",
    "        # Loss plot\n",
    "        ax1.plot(self.train_losses, label='Training Loss')\n",
    "        ax1.plot(self.val_losses, label='Validation Loss')\n",
    "        ax1.set_title('Training and Validation Loss')\n",
    "        ax1.set_xlabel('Epoch')\n",
    "        ax1.set_ylabel('Loss')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True)\n",
    "        \n",
    "        # Accuracy plot\n",
    "        ax2.plot(self.train_accuracies, label='Training Accuracy')\n",
    "        ax2.plot(self.val_accuracies, label='Validation Accuracy')\n",
    "        ax2.set_title('Training and Validation Accuracy')\n",
    "        ax2.set_xlabel('Epoch')\n",
    "        ax2.set_ylabel('Accuracy (%)')\n",
    "        ax2.legend()\n",
    "        ax2.grid(True)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('clm_training_history.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "\n",
    "def analyze_feature_correlations(model: CorrelationLearningMechanism, dataloader: DataLoader, device: torch.device):\n",
    "    \"\"\"\n",
    "    Analyze feature correlations learned by the CLM\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_features = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, targets in dataloader:\n",
    "            data, targets = data.to(device), targets.to(device)\n",
    "            features, _ = model(data)\n",
    "            all_features.append(features.cpu().numpy())\n",
    "            all_labels.append(targets.cpu().numpy())\n",
    "    \n",
    "    all_features = np.vstack(all_features)\n",
    "    all_labels = np.hstack(all_labels)\n",
    "    \n",
    "    print(f\"Extracted features shape: {all_features.shape}\")\n",
    "    \n",
    "    # Calculate feature correlations\n",
    "    correlation_matrix = np.corrcoef(all_features.T)\n",
    "    \n",
    "    # Plot correlation matrix\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    plt.imshow(correlation_matrix, cmap='coolwarm', aspect='auto')\n",
    "    plt.colorbar()\n",
    "    plt.title('Feature Correlation Matrix')\n",
    "    plt.xlabel('Feature Index')\n",
    "    plt.ylabel('Feature Index')\n",
    "    plt.savefig('feature_correlations.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    return all_features, all_labels\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run feature extraction and correlation learning\"\"\"\n",
    "    # Configuration\n",
    "    DATA_DIR = \"/content/Class_project_Early_detection_of_brain_tumor/CT_enhanced\"\n",
    "    BATCH_SIZE = 16\n",
    "    EPOCHS = 50\n",
    "    \n",
    "    # Set device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = BrainTumorDataset(DATA_DIR, split=\"train\")\n",
    "    val_dataset = BrainTumorDataset(DATA_DIR, split=\"val\")\n",
    "    test_dataset = BrainTumorDataset(DATA_DIR, split=\"test\")\n",
    "    \n",
    "    print(f\"Training samples: {len(train_dataset)}\")\n",
    "    print(f\"Validation samples: {len(val_dataset)}\")\n",
    "    print(f\"Test samples: {len(test_dataset)}\")\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "    \n",
    "    # Initialize CLM model and move to device immediately\n",
    "    model = CorrelationLearningMechanism(input_channels=4, num_classes=2)\n",
    "    model = model.to(device)  # Move model to device before any operations\n",
    "    print(f\"Model initialized with {sum(p.numel() for p in model.parameters()):,} parameters\")\n",
    "    \n",
    "    # Test a forward pass with a single batch to verify the fix\n",
    "    print(\"Testing forward pass with a single batch...\")\n",
    "    with torch.no_grad():\n",
    "        test_batch, test_targets = next(iter(train_loader))\n",
    "        test_batch = test_batch.to(device)\n",
    "        # Ensure model is on the same device\n",
    "        model = model.to(device)\n",
    "        features, outputs = model(test_batch)\n",
    "        print(f\"Forward pass successful! Features shape: {features.shape}, Outputs shape: {outputs.shape}\")\n",
    "    \n",
    "    # Initialize trainer (model is already on device)\n",
    "    trainer = FeatureExtractionTrainer(model, device)\n",
    "    \n",
    "    # Train the model\n",
    "    trainer.train(train_loader, val_loader, epochs=EPOCHS)\n",
    "    \n",
    "    # Plot training history\n",
    "    trainer.plot_training_history()\n",
    "    \n",
    "    # Analyze feature correlations\n",
    "    print(\"Analyzing feature correlations...\")\n",
    "    features, labels = analyze_feature_correlations(model, val_loader, device)\n",
    "    \n",
    "    # Load best model for final evaluation\n",
    "    checkpoint = torch.load('best_clm_model.pth', map_location=device)  # Specify device when loading\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model = model.to(device)  # Ensure model is on correct device after loading\n",
    "    \n",
    "    # Final evaluation on test set\n",
    "    test_loss, test_accuracy = trainer.validate_epoch(test_loader)\n",
    "    print(f\"\\nFinal Test Results:\")\n",
    "    print(f\"Test Loss: {test_loss:.4f}\")\n",
    "    print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
    "    \n",
    "    print(\"\\n‚úÖ Feature Extraction and Correlation Learning completed successfully!\")\n",
    "    print(\"üìä Model saved as: best_clm_model.pth\")\n",
    "    print(\"üìà Training history saved as: clm_training_history.png\")\n",
    "    print(\"üîç Feature correlations saved as: feature_correlations.png\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
